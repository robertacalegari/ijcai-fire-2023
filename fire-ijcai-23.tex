%%%% ijcai23.tex

\typeout{IJCAI--23 Instructions for Authors}

% These are the instructions for authors for IJCAI-23.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai23.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai23}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{color}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{cleveref}
\usepackage[inline]{enumitem}
\usepackage{subfig}

\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	%numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true,
	keywordstyle=\color{red},
	keywords={virginica,versicolor,setosa}}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}

\newcommand{\continue}{\textbf{continue}}
\newcommand{\undefined}{\textbf{undefined}}
\newcommand{\New}[1]{\textbf{new} #1}
%\newcommand{\Comment}[1]{$\triangleright$ #1}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\avg}{avg}
\DeclareMathOperator*{\union}{\bigcup}

\newcommand{\xai}{XAI}
\newcommand{\psyke}{omitted}%PSyKE
\newcommand{\real}{\textsc{REAL}}
\newcommand{\trepan}{\textsc{Trepan}}
\newcommand{\iter}{\textsc{Iter}}
\newcommand{\gridex}{GridEx}
\newcommand{\gridrex}{GridREx}
\newcommand{\cream}{\textsc{CREAM}}
\newcommand{\creepy}{CReEPy}
\newcommand{\crash}{\textsc{CRASH}}
\newcommand{\pedro}{\textsc{PEDRO}}
\newcommand{\cart}{\textsc{Cart}}
\renewcommand{\smile}{Smile}
\newcommand{\scikit}{Scikit-Learn}
\newcommand{\exact}{ExACT}
\newcommand{\fire}{FiRe}
\newcommand{\psifire}{$\psi$-\fire}

\newcommand\twoinarow{0.49\linewidth}
\newcommand\twoinarowlarge{0.51\linewidth}
\newcommand\threeinarow{.3375\linewidth}
\newcommand\fourinarow{0.245\linewidth}

\usepackage[inline]{enumitem}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{cleveref}

\newenvironment{inlinelist}{\begin{enumerate*}[label=\emph{(\roman{*})}]}{\end{enumerate*}}

% Comment out this line in the camera-ready submission
\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.
% Please **do not** include Title and Author information
\pdfinfo{
/TemplateVersion (IJCAI.2023.0)
}

\title{Symbolic Knowledge-Extraction Evaluation Metrics: The \fire{} Score}


% Single author syntax
\author{
    Anonymous submission
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
%\iffalse
%\author{
%First Author$^1$
%\and
%Second Author$^2$\and
%Third Author$^{2,3}$\And
%Fourth Author$^4$
%\affiliations
%$^1$First Affiliation\\
%$^2$Second Affiliation\\
%$^3$Third Affiliation\\
%$^4$Fourth Affiliation
%\emails
%\{first, second\}@example.com,
%third@other.example.com,
%fourth@example.com
%}
%\fi

\begin{document}

\maketitle

\begin{abstract}
Symbolic knowledge-extraction techniques are becoming of key importance for AI applications since they enable the explanation of opaque black-box predictors, enhancing trust and transparency.
%
Existing techniques use to require tuning of hyper-parameters. Manual tuning is too time-expensive for users, who conversely could benefit from automatic procedures to perform the task.
%
However, automatic procedures can compare different extraction algorithms only if an adequate metric -- such as a scoring function resuming all the interesting features of the extractors -- is provided.
%
The definition of an evaluation metric for symbolic knowledge extractors is currently missing in the literature.

Accordingly, in this paper we introduce the \fire{} score metric to assess the quality of a symbolic knowledge-extraction procedure, taking into account both its predictive performance and the readability of the extracted knowledge.
%
A rigorous mathematical formulation is provided along with several practical examples to highlight its effectiveness to the end of being exploited inside automatic hyper-parameter tuning procedures.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%
One of the main strengths of machine learning (ML) models is their ability to provide exceptionally accurate predictions when applied in (roughly) every conceivable scenario~\cite{rocha2012far}.
%
Unfortunately, the most powerful ML predictors (such as deep neural networks, for instance) present a high price in terms of human interpretability of their outputs.
%
Indeed, they acquire knowledge during a training phase and store it in a sub-symbolic way, in the form of internal parameters.
%
This common \emph{opaque} behaviour constitutes a real barrier to the exploitation of such models, named \emph{black boxes} (BBs), in critical areas, that are all those real-world applications heavily impacting human lives, e.g., in terms of safety, health, and finance.

Different solutions have been proposed by the explainable artificial intelligence community to combine human interpretability with the predictive performance of BB models~\cite{guidotti2018survey}.
%
Amongst the strategies available in the literature there is the choice of intrinsic explainable models~\cite{Rudin2019}, such as decision trees with a limited amount of internal nodes and leaves.
%
When this option is not feasible or does not provide satisfying results, a different research branch suggests extracting the BB acquired knowledge by adhering to some \emph{symbolic} representation, through a reverse-engineering of the BB behaviour~\cite{KENNY2021103459}.
%
This second strategy is the rationale behind symbolic knowledge-extraction (SKE) procedures.

In the years, a plethora of SKE techniques has been proposed in the literature, especially to tackle supervised classification tasks.
%
Given the amount of available analogous algorithms applicable to the same tasks, it may be complex to find the most suitable.
%
Furthermore, some procedures need the fine tuning of a set of hyper-parameters, usually requiring time and skills to be performed by users.

Comparisons between different instances of the same extractor, or different extractors, are usually carried out by observing
%
\begin{inlinelist}
	\item the predictive performance of the extractor, w.r.t.\ both the underlying BB predictions and the actual data set output variables; and
	\item the readability of the output human-intelligible knowledge.
\end{inlinelist}

The former can be easily assessed via the same metrics adopted to measure the predictive performance of the underlying BB (e.g., F$_1$ and accuracy scores for classification tasks and mean absolute/squared error and R$^2$ score for regression tasks).
%
Conversely, the latter may be measured through different indicators, however, to the best of our knowledge, a widely-acknowledged, well-founded, and sound definition has not been yet formulated.
%
Comparisons performed by users, as well as automated algorithmic comparisons, can surely benefit from a unified scoring function encompassing both concepts of predictive performance and readability associated with the knowledge provided by SKE techniques.
%
\textbf{....RC: aggiungi che andiamo verso AutoML....molto molto usato oggi}
%
Accordingly, in this paper we propose the \fire{} score as a compact and expressive metric to evaluate and compare different knowledge extractors, also in association with automated parameter tuning procedures.

The paper is organized as follows....\textbf{....RC: TBC}...

%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivations}\label{sec:motivation}
%%%%%%%%%%%%%%%%%%%%%%%

SKE techniques have been applied in a wide variety of areas~\cite{steiner2006using,hayashi2000comparison,sabbatini22LPFSKE}.
%%
%The process of knowledge extraction from opaque ML models can follow different paradigms, i.e., SKE algorithms can be either decompositional or pedagogical~\cite{andrews1995survey}.
%%
%In both cases, the output knowledge is generally represented as a set of logic rules, sometimes translated into natural language.
%%
%Decompositional techniques take into account the internal structure of the BB, that in turn is related to the BB nature.
%%
%For instance, decompositional algorithms suitable for artificial neural networks cannot be applied to support-vector machines.
%%
%Pedagogical techniques, on the other hand, are more general, since they are applicable to any kind of opaque model.
%%
%They only consider the output response corresponding to a given input instance or set of instances, with the aim of creating a mimicking, human-interpretable model able to approximate the underlying model predictions in the most adherent way.
%
Comparisons between different instances of the same extractor, or different extractors, in order to select the ``best knowledge extracted" are usually carried out by measuring
%
\begin{inlinelist}
	\item the predictive performance of the extractor, w.r.t.\ both the underlying BB predictions and the actual data set output variables; and
	\item the readability of the extracted human-intelligible knowledge.
\end{inlinelist}
%
However, such a comparison in the literature is always done manually, just looking at the predictive performance of the extractor and the readability, separately.
So, to the best of our knowledge, not only an automatic way of evaluation is missing in the literature, but also the definition of a metric to synergically combine predictive performance and readability to measure the quality of the extracted knowledge as a unique indicator.

 while the definition of a metric for automatic evaluation is still missing. 

The predictive performance of the extracted knowledge may be assessed w.r.t.\ 2 different dimensions by using the same scoring function adopted for the underlying BB.
%
These dimensions are:
%
\begin{inlinelist}
	\item the mimicking capabilities w.r.t.\ the underlying model predictions, usually called \emph{fidelity}; and
	\item the \emph{predictive performance} w.r.t.\ the data set output features.
\end{inlinelist}

The readability from a human perspective is usually done by comparing the number of items extracted (i.e., an algorithm that extracts 5 rules is considered more readable than an algorithm that extracts 10 rules; or an algorithm that extracts a decision tree with depth 5 is considered more readable than an algorithm extracts a decision tree with depth 10) \cite{literature}.
% 
It is evident that an evaluation made only at this level can be considered rather superficial, since a wider set of indicators should be taken into account for a fair evaluation \cite{sabbatini2022-metrics}.
%
In particular, a readability metric should at least consider:
%
\begin{inlinelist}
	\item the shape of the extracted knowledge, e.g., list or trees of rules, decision tables, etc.; and
	\item the readability of single atoms composing the knowledge, e.g., how individual rules or tree nodes and leaves are constructed.
\end{inlinelist}
%
However, both these items may require further investigations to discern between more and less readable knowledge representations and how to formalize them.

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On the knowledge readability: considerations and examples}\label{sec:motivation}
%%%%%%%%%%%%%%%%%%%%%%%
As mentioned above, knowledge extracted from a BB via SKE comes in the form of lists, decision trees, logical rules, or decision tables \cite{aggiungere}. 
%
In the following, some considerations and examples of different levels of readability of these outputs are discussed, to make the reader understand all the features that a readability evaluation metric should take into account when formulated.

It is worth noting that rule lists and trees are equivalent, since starting from a tree it is possible to build a list of rules by converting each path from the tree root to a different leaf into a distinct rule.
%
Analogously, decision tables usually represent a rule per row (or column), so they can be easily translated into a list of rules. 
%
Hence, all the different output formats in terms of shape are traceable to each other to be compared fairly.

There exist remarkable differences, however, between \emph{ordered} and \emph{unordered} lists, since in the first case rules are evaluated from the top to the bottom, and for this reason, bottom rules may be simplified by assuming as trivially false the conditions represented in the top rules.
%
This leads to more concise knowledge representations, but at the cost of human readability, since to interpret the output of the bottommost rule it is necessary to acknowledge as false all the others.
%
As a consequence, it is evident how the mere amount of output rules is a rough but reliable indicator of readability, but the readability of single rules presents tricky challenges that should be carefully investigated.

Furthermore, rules (that comes usually in form of logic rules) are usually implications having a set of preconditions (e.g., describing an input feature space subregion) and a postcondition (i.e., the output associated with inputs belonging to the subregion).
%
Preconditions and postconditions present different degrees of intrinsic readability.
%
In particular, examples of preconditions are {con\-junc\-tions}/disjunctions of \emph{if-then}, \emph{M-of-N}, oblique or fuzzy rules, in positive or negative form.
%
Also in this case the more compact representations are denoted by a smaller human readability and it is a difficult task to assign a numerical readability score to them.
%
For instance, a trivial count of conditions and/or constants and/or variables contained in the rules is not suitable to assess readability, and this is easily demonstrated by considering the following example.
%
A precondition expressed as
%
\begin{equation*}
	if (X \geq 0.5) \wedge (X \leq 0.75)
\end{equation*}
%
contains one variable, two conditions, and two constants.
%
An equivalent fuzzy rule in the form 
%
\begin{equation*}
	if~X~is~medium
\end{equation*}
%
has one variable as well, but only one condition and one constant.
%
However, it presents the same degree of readability for humans, that need to know the semantics of $medium$.
%
Finally, the same concept may be modelled as 
%
\begin{equation*}
	if~X \in [0.5, 0.75],
\end{equation*}
%
differing from the first representation in the number of conditions (1 vs. 2).
%
But, actually, the readability extent is absolutely equivalent.

On the other hand, as a final consideration, postconditions may be constant values or some kind of function involving input features.
%
In the latter case, readability is obviously reduced.

These trivial examples show how quantitatively measuring the readability of "extracted knowledge" is not so trivial, and also leads to non-quantitative and subjective parameters to be taken into consideration. For this reason, in this work, we will limit the metric to take into account only the number of rules as an index of human interpretability---which is, in any case, what is reflected in the literature today.

%%%%%%%%%%%%%%%%%%%%%%%
\section{The \fire{} score}\label{sec:fire}
%%%%%%%%%%%%%%%%%%%%%%%

Given all the observations discussed above, the scoring function presented in the following only relies on the amount of extracted rules as a readability indicator, since it appears to be the most straightforward and unambiguous.
%
So the unique metric formulated for measuring the quality of the knowledge extracted via SKE will take into account both the predictive performance and the readability intended as human interpretability.

\captionsetup[subfigure]{width=.4\linewidth}
\begin{figure}[tb]\centering
	\subfloat[1-\fire{}.]{
		\includegraphics[width=0.45\linewidth]{figures/psi_1_color.pdf}\label{fig:psi1}
	}
	\subfloat[3-\fire{}.]{
		\includegraphics[width=0.45\linewidth]{figures/psi_3_color.pdf}\label{fig:psi3}
	}
	\\
	\subfloat[5-\fire{}.]{
		\includegraphics[width=0.45\linewidth]{figures/psi_5_color.pdf}\label{fig:psi5}
	}
	\subfloat[10-\fire{}.]{
		\includegraphics[width=0.45\linewidth]{figures/psi_10_color.pdf}\label{fig:psi10}
	}
	\caption{Graphs of different \psifire{} scoring functions, having $\psi \in \{1, 3, 5, 10\}$.}\label{fig:psi}
\end{figure}

The \fire{} score is a multivariate function defined as follows:
%
\begin{align}
	&\fire: \left( \mathbb{R}_{>0} \times \mathbb{R}_{\geq 0} \times \mathbb{R}_{\geq1} \right) \mapsto \mathbb{R}_{\geq 0},\\
	&\fire(\psi, p, r) = p \left \lceil{\frac{r}{\psi}}\right \rceil r^{0.05},
\end{align}
%
where $\psi$ is the fidelity/readability trade-off extent, $p$ is a measure of the predictive loss of the extractor and $r$ is a measure of its readability loss.

%-----------------------------------------------------%
\subsection{Variables and parameters}\label{sec:fire-var}
%-----------------------------------------------------%
As for the predictive loss $p$, A good measure for the predictive loss $p$ in regression tasks is the mean absolute error (MAE) of the extractor's predictions w.r.t.\ the underlying BB predictions or the data set outputs, depending on the need.
%
For classification tasks, it is possible to use metrics anti-correlated with the accuracy score, e.g., $1-accuracy$.
%
The presented \fire{} score adopts the mean absolute error metrics for regression, but it may be substituted with the mean squared error without substantial differences since both of them are generally correlated.
%
Analogously, metrics inversely proportional to the R$^2$ value may be also exploited.

As for the readability loss $r$, the total amount of output rules is a suitable metric and thus it is the one adopted to calculate the \fire{} score.
%
More complex options will be evaluated in the future, e.g., taking into account the complexity of individual rules.

Finally, the $\psi$ parameter -- the only user-defined parameter -- describes how much the predictive loss can be penalised w.r.t.\ the readability loss.
%
It is important because, depending on the task at hand, the two losses may have different weights.
%
In particular, $\psi=1$ assigns the same importance to both losses.
%
Growing $\psi$ values tend to neglect the readability loss impact.
%
In other words, given the aforementioned readability loss formulation and by assuming to have extracted $m$ rules, if users set $\psi=n$ the \fire{} score will consider only $\frac{m}{n}$ rules, rounded up to the nearest integer.

%-----------------------------------------------------%
\subsection{Function domain}\label{sec:fire-domain}
%-----------------------------------------------------%
The domain of the function is explained by the following observations.
%
The $\psi$ parameter is a positive real value for design.
%
Limiting the admissible $\psi$ values to $\mathbb{N}_1$ may be also reasonable, but an extension to $\mathbb{R}_{>0}$ makes the \fire{} score more flexible.
%
On the other hand, the $p$ parameter is a measurement of a predictive error, so it may be equal to 0 in the best case, or arbitrarily larger otherwise since there is no upper bound to the predictive error of a model.
%
Finally, $r$ is an integer number greater or equal to 1, since it represents a discrete quantity.
%
However, the admissible values for this parameter have been extended from $\mathbb{N}_1$ to $\mathbb{R}_{\geq 1}$ for the sake of flexibility, analogously to the range for $\psi$.
%
This choice enables, for instance, the \fire{} score calculation for averaged sets of extractors trained with the same hyper-parameters, resulting in a more robust score.
%
Similarly to the $p$ parameter, there is no upper bound for $r$.

As a result of the observations above, the \fire{} score is defined as a continuous (yet non-differentiable) function in the aforementioned domain and it may assume any non-negative value.
%
Therefore, the score is a function bounded from below by 0.

%-----------------------------------------------------%
\subsection{Score meaning}\label{sec:fire-meaning}
%-----------------------------------------------------%
\textbf{formula explanation in words: why multiplication? Why a division? Why 0.05?...ALL: give the intuition behind}

In the following, we use the notation \psifire{}($p$, $r$) as a clearer alias of \fire{}($\psi$, $p$, $r$) and we consider without loss of generality the \psifire{}($\cdot$) function as a bivariate function, by assuming the $\psi$ parameter fixed \emph{a priori}.

\begin{figure*}[tb]\centering
	\subfloat[Trend of 5-\fire{} w.r.t.\ readability loss for different predictive loss values.]{
		\includegraphics[width=\twoinarow]{figures/readVSfire.pdf}\label{fig:readVSfire}
	}
	\subfloat[Trends of several \psifire{} functions having equal predictive loss w.r.t.\ readability loss.]{
		\includegraphics[width=\twoinarow]{figures/readVSfirePSI.pdf}\label{fig:readVSfirePSI}
	}
	\\
	\subfloat[Trend of 5-\fire{} w.r.t.\ predictive loss for different readability loss values.]{
		\includegraphics[width=\twoinarow]{figures/maeVSfire.pdf}\label{fig:maeVSfire}
	}
	\subfloat[Trends of several \psifire{} functions having equal readability loss w.r.t.\ predictive loss.]{
		\includegraphics[width=\twoinarow]{figures/maeVSfirePSI.pdf}\label{fig:maeVSfirePSI}
	}
	\caption{Projections of several \psifire{} functions w.r.t.\ different values of $\psi$, readability loss and predictive loss.}\label{fig:proj}
\end{figure*}

The \fire{} score has been formulated to assign low scores to desirable extractors.
%
It assumes that a good extractor should exhibit a low predictive loss and a low readability loss.
%
For this reason, it is a multiplicative score between the two parameters.
%
The ceiling function appearing as the second factor of the score has the purpose to give a step-function shape to the \fire{} score.
%
The exact shape of the steps is regulated through the $\psi$ parameter.
%
By setting $\psi=n$, users impose these steps to have a length equal to $n$.
%
Since a flat step would assign the same \psifire{} score to extractors having the same predictive loss and a different but similar readability loss (e.g., $r$ = 1 and 2, respectively, and $\psi=10$), a third factor is queued to the score definition to discern amongst the extractors lying on the same step which one has to be considered the best.
%
In this way the \fire{} score keeps the step-function shape but becomes an increasing monotonic function (for any $p>0$, since $\psi\textrm{-}\fire{}(0, r)=0, \forall r, \forall \psi$).
%
Examples of \psifire{} graphs are reported in \Cref{fig:psi}, for different values of $\psi$, $p$ and $r$.

%-----------------------------------------------------%
\subsection{Properties}\label{sec:fire-prop}
%-----------------------------------------------------%
The monotonicity of the \psifire{} score is ensured by the following conditions:
\begin{description}
	\item[monotonicity w.r.t.\ the projection of $p$] (cf.\ \Cref{fig:readVSfire})
	\begin{equation}
		\begin{split}
			r_1 < r_2 \iff &\psi\textrm{-}\fire(p, r_1) < \psi\textrm{-}\fire(p, r_2), \\ &\forall p \in \mathbb{R}_{>0}, ~~~\forall r_1, r_2 \in \mathbb{R}_{\geq1}
		\end{split}\label{eq:projP}
	\end{equation}
	\item[monotonicity w.r.t.\ the projection of $r$] (cf.\ \Cref{fig:maeVSfire})
	\begin{equation}
		\begin{split}
			p_1 < p_2 \iff &\psi\textrm{-}\fire(p_1, r) < \psi\textrm{-}\fire(p_2, r), \\ &\forall r \in \mathbb{R}_{\geq1}, ~~~\forall p_1, p_2 \in \mathbb{R}_{>0}
		\end{split}\label{eq:projR}
	\end{equation}
\end{description}
%
Alternatively, \Cref{eq:projP,eq:projR} can be substituted by the following condition:
%
\begin{description}
	\item[monotonicity w.r.t.\ a partial order on the domain] 
	\begin{equation}
		\begin{split}
			&(p_1 < p_2) \wedge (r_1 < r_2) \iff \\&\iff \psi\textrm{-}\fire(p_1, r_1) < \psi\textrm{-}\fire(p_2, r_2), \\ &\forall p_1, p_2 \in \mathbb{R}_{>0}, ~~~\forall r_1, r_2 \in \mathbb{R}_{\geq1}
		\end{split}\label{eq:partial}
	\end{equation}
\end{description}

The increasing trend of the score may be observed in \Cref{fig:proj} and it is demonstrated through its partial derivatives.
%
\Cref{eq:projP,eq:projR,eq:partial} hold for any possible $\psi>0$ that may be assigned to the \psifire{} score, as it is possible to notice from \Cref{fig:readVSfirePSI,fig:maeVSfirePSI}.

The partial derivative w.r.t.\ $p$ is the following:
%
\begin{equation}
	\frac{\partial \psi\textrm{-}\fire}{\partial p} = \left \lceil{\frac{r}{\psi}}\right \rceil r^{0.05} \label{eq:partialP}
\end{equation}
%
always positive and defined in the whole domain.
%
The partial derivative w.r.t.\ $r$ is the following:
%
\begin{equation}
	\frac{\partial \psi\textrm{-}\fire}{\partial r} = \frac{0.05 p \left \lceil{\frac{r}{\psi}}\right \rceil}{r^{0.95}}\label{eq:partialR}
\end{equation}
%
always positive for $p>0$ and defined in the whole domain except for $\frac{r}{\psi} \in \mathbb{Z}$.
%
The derivative is 0 for $p=0$, indeed in this case the \psifire{} score is always 0 regardless of the values of $\psi$ and $r$.

%%%%%%%%%%%%%%%%%%%%%%%
\section{About \fire{}: some considerations}
%%%%%%%%%%%%%%%%%%%%%%%
\textbf{aggiungere un cappello alle due sezioni: cosa vogliamo far vedere spiegare in a nutshell?}

\subsection{Comparing Algorithms with \fire{}}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/boundaries.pdf}
	\caption{Graphical representation of the boundaries identified by the 1-\fire{} score (isoline for $1\textrm{-}\fire{}=40.0$).}\label{fig:boundaries}
\end{figure}

Given all the aforementioned properties about the \fire{} scoring function, we exemplify here some applicative scenarios from a theoretical point of view.
%
Let us assume to have an extraction procedure providing as output knowledge a single human-interpretable rule.
%
The mean absolute error associated with this rule is equal to 40.0 and we chose to adopt $\psi=1$.
%
As a consequence, $1\textrm{-}\fire(40.0, 1)=40.0$.

In \Cref{fig:boundaries} the isoline corresponding to a 1-\fire{} score equal to 40.0 is represented in red.
%
Readability loss and predictive loss are reported on the x-axis and y-axis, respectively, as the number of extracted rules and mean absolute error.
%
The extractor under study, with 1 rule and a predictive error equal to 40.0, lies on the red isoline.
%
The same condition holds for all extractors having the same 1-\fire{} score value.
%
This is the case, for example, of extractors providing 2, 4, 6 and 8 rules associated with MAE of 19.3, 9.3, 6.1 and 4.5, respectively.
%
All these models are considered \emph{equivalent} on the basis of the 1-fire{} score.
%
Conversely, a model able to extract 4 rules with a predictive error of 5.0 is considered \emph{better}, since it has a smaller 1-\fire{} score and thus it lies in the graph under the red isoline.
%
More precisely, $1\textrm{-}\fire(5.0, 4)=21.4$.
%
On the other hand, an extractor providing 6 rules with MAE = 12.0 is considered \emph{worse}, since its 1-\fire{} score is greater than 40.0 and thus it graphically lies above the red isoline.
%
Indeed, $1\textrm{-}\fire(12.0, 6)=78.7$.

\Cref{fig:boundaries} clearly highlights how the \fire{} score identifies an exact boundary separating, w.r.t.\ a given extractor, the sets of equivalent, worse and better extractors.
%
Furthermore, the isoline depicts the fidelity/readability trade-off correlated to $\psi=1$.
%
%Indeed, this is the smallest admissible value for the parameter, and it is the value assigning the maximum importance to readability losses.
%
By observing the red isoline it is noticeable how a doubling of the readability loss (e.g., from 2 to 4) is accepted only if it is approximately balanced with a halving of the predictive loss.
%
The curve can be also read in the opposite sense, e.g., a doubling of the predictive loss is accepted only when (approximately) compensated by a readability loss halving.

Finally, we exploit the same Figure to stress the fact that the isoline presents an asymptotic trend when the $p$ and $r$ parameters tend to infinity.
%
This behaviour reflects the actual quality of the knowledge provided by SKE techniques.
%
Indeed, when the number of rules or the predictive error are very high, the evaluated knowledge has low quality and it is no more a relevant task to have a fine-grained measure of \emph{how} a loss should be compensated by the other.

\subsection{On the selection of the $\psi$ Parameter}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/areas.pdf}
	\caption{Different boundaries associated to the 2-\fire{} and 6-\fire{} scores.}\label{fig:areas}
\end{figure}

It is of fundamental importance to carefully choose the $\psi$ parameter of \fire{}.
%
\Cref{fig:readVSfirePSI} already shown that larger values of $\psi$ reduce the impact of the readability loss.
%
However, it is important to know that different $\psi$ values may lead to \emph{opposite} results when applied to compare the same extractors.
%
This peculiarity is depicted in \Cref{fig:areas}, representing the separating boundaries identified by the isolines obtained via the 2-\fire{} and 6-\fire{} scores w.r.t.\ a given extractor described in the following.
%
The boundaries associated with the two scoring functions are represented as green and red isolines, respectively.
%
The hatched area below each isoline highlights the parameter space region denoting ``more desirable'' extractors, providing knowledge with better quality w.r.t.\ extractors lying on the isoline.

Let us assume to have an extractor able to obtain 4 rules from a BB model with a mean absolute error equal to 1.0 (blue cross in \Cref{fig:areas}).
%
The \psifire{} scores associated with this model are:
%
\begin{equation*}
	2\textrm{-}\fire(1.0, 4)=2.14,
\end{equation*}
%
\begin{equation*}
	6\textrm{-}\fire(1.0, 4)=1.07.
\end{equation*}
%
An SKE algorithm extracting 8 rules with MAE = 0.5 (orange cross) has the same \fire{} scores for both values of $\psi$.
%
The models are thus equivalent according to both of the considered scoring functions.
%
Analogously, by assuming two extractors providing 2 and 3 output rules with predictive errors equal to 0.5 and 2.0, respectively (fuchsia and purple cross in the Figure), both scores are unanimous in evaluating the former as a better extraction procedure and in considering worse the latter.

Different behaviours can be observed, for instance, by selecting an extracted knowledge composed of a single rule with MAE = 1.5 (green cross).
%
In this case, the scores are evaluated as follows:
%
\begin{equation*}
	2\textrm{-}\fire(1.5, 1)=1.5 < 2.14 = 2\textrm{-}\fire(1.0, 4),
\end{equation*}
%
\begin{equation*}
	6\textrm{-}\fire(1.5, 1)=1.5 > 1.07 = 6\textrm{-}\fire(1.0, 4),
\end{equation*}
%
and their interpretation leads to opposite conclusions.
%
In particular, the single-ruled knowledge is considered better than the others lying on the isoline if considering $\psi=2$.
%
On the other hand, it is worse if considering $\psi=6$.

The dual situation can be encountered with knowledge having 5 rules and MAE = 0.8 (red cross).
%
In this case the knowledge quality is considered better when evaluated through the 6-\fire{} score and worse with the 2-\fire{} score.

Given all these remarks, we suggest selecting the most adequate value of the $\psi$ parameter for the task at hand after having observed the corresponding isolines.

%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments and discussion}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%

\captionsetup[subfigure]{width=.25\linewidth}
\begin{figure*}[tb]\centering
	\subfloat[9-NN.]{
	\includegraphics[width=\fourinarow]{figures/classif/knn.pdf}\label{fig:knn}
	}
	\subfloat[\cart{}.]{
		\includegraphics[width=\fourinarow]{figures/classif/knn-cart.pdf}\label{fig:cart}
	}
	\subfloat[\iter{}.]{
		\includegraphics[width=\fourinarow]{figures/classif/knn-iter.pdf}\label{fig:iter}
	}
	\subfloat[\creepy{} (4 features).]{
		\includegraphics[width=\fourinarow]{figures/classif/knn-creepy2.pdf}\label{fig:creepy2}
	}
	\\
	\subfloat[\creepy{} (2 features).]{
		\includegraphics[width=\fourinarow]{figures/classif/knn-creepy1.pdf}\label{fig:creepy1}
	}
	\subfloat[\gridex{} (3 rules).]{
		\includegraphics[width=\fourinarow]{figures/classif/knn-gridex3.pdf}\label{fig:gridex3}
	}
	\subfloat[\gridex{} (4 rules).]{
		\includegraphics[width=\fourinarow]{figures/classif/knn-gridex4.pdf}\label{fig:gridex4}
	}
	\subfloat[\gridex{} (6 rules).]{
		\includegraphics[width=\fourinarow]{figures/classif/knn-gridex8.pdf}\label{fig:gridex8}
	}
	\\
	\subfloat[1-\fire{}.]{
		\includegraphics[width=\threeinarow]{figures/exp1.pdf}\label{fig:phi1}
	}
	\subfloat[2-\fire{}.]{
		\includegraphics[width=\threeinarow]{figures/exp2.pdf}\label{fig:phi2}
	}
	\subfloat[3-fire{}.]{
		\includegraphics[width=\threeinarow]{figures/exp3.pdf}\label{fig:phi3}
	}
	\caption{Decision boundaries for the Iris data set obtained with different extractors applied to a 9-NN and corresponding 1-\fire{}, 2-\fire{} and 3-\fire{} score isolines.}\label{fig:experiments}
\end{figure*}

\begin{table}[t]\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\toprule
		Algorithm & Accuracy, $p$ & $r$ & \multicolumn{3}{c|}{\psifire{}} \\
		& & & $\psi=1$ & $\psi=2$ & $\psi=3$ \\
		\midrule\midrule
		9-NN & 0.97,~~~~-~~~ & - & - & - & - \\
		\cart{} & 0.95, 0.05 & 3 & 0.17 & 0.11 & 0.06 \\
		\iter{} & 0.89, 0.11 & 3 & 0.36 & 0.24 & 0.12 \\
		\creepy{} & 0.93, 0.07 & 3 & 0.22 & 0.14 & 0.07 \\
		(4 feat.) & & & & & \\
		\creepy{} & 0.91, 0.09 & 3 & 0.27 & 0.18 & 0.09 \\
		(2 feat.) & & & & & \\
		\gridex{} & 0.94, 0.06 & 3 & 0.18 & 0.12 & 0.06 \\
		\gridex{} & 0.81, 0.19 & 4 & 0.80 & 0.40 & 0.40 \\
		\gridex{} & 0.97, 0.03 & 8 & 0.26 & 0.13 & 0.11 \\
		\bottomrule
	\end{tabular}
	\caption{Quality assessments for the knowledge extracted by different SKE algorithms from a 9-NN for the Iris data set.}
	\label{tab:experiments}
\end{table}

The effectiveness of the \fire{} score to evaluate and compare the quality of SKE techniques' extracted knowledge has been assessed by running several experiments.
%
In particular, the \psyke{} framework~\cite{putCitations} has been used to train a BB predictor and a set of extractors on the well-known Iris dataset\footnote{\url{https://archive.ics.uci.edu/ml/datasets/iris}}~\cite{iris}.%\cite{psyke-woa2021,psyke-ia2022}
%
The adopted extractors are the following: \cart{}~\cite{breiman1984classification}, \iter{}~\cite{huysmans2006iter}, \creepy{}~\cite{putCitations} and \gridex{}~\cite{gridex-extraamas2021}.
%
All these techniques have been applied to a $k$-nearest neighbour ($k$-NN) classifier, having $k=9$.
%
Since they all are pedagogical algorithms, the provided output knowledge has been extracted only by observing the input/output response of the 9-NN.

\cart{} induces a decision tree classifier on the 9-NN predictions and it has been executed with the default parameters.

On the other hand, \iter{}, \creepy{} and \gridex{} produce a hypercubic partitioning of the input feature space according to different strategies.
%
\iter{} creates and expands cubes in a bottom-up iterative fashion.
%
It relies on 4 hyper-parameters:
%
\begin{inlinelist}
	\item the number of starting cubes, set to 1;
	\item the minimum amount of instances to consider inside each cube, set to 75;
	\item the size of cube updates, set to 7\% of each input feature range interval;
	\item the maximum number of iterations to be performed, set to 600.
\end{inlinelist}

\gridex{} partitions the input feature space in a top-down recursive and symmetric manner, starting from the whole space.
%
It relies on 4 hyper-parameters:
%
\begin{inlinelist}
	\item the maximum depth of the recursive splitting;
	\item the minimum amount of instances to consider inside each cube, set to 1;
	\item the number of slices to perform at each iteration;
	\item the error threshold to decide if a hypercubic region should be further partitioned, set to 0.1.
\end{inlinelist}
%
An error threshold equal to 0.1 means that all cubes having an accuracy smaller than 0.9 are further split.
%
The number of slices to perform has been adaptively chosen.
%
In particular, our experiments, resumed in \Cref{tab:experiments}, consider 3 \gridex{} instances.
%
The first and the second perform 8 and 2 slices, respectively, only along the most relevant input feature.
%
The third performs 4 slices on the 2 most relevant input dimensions.
%
As for the maximum depth, the first instance has a value equal to 1, and the others equal to 2.

\creepy{} adopts an underlying clustering technique to divide the input space into hypercubic hierarchical regions.
%
We set equal to 2 the maximum depth parameter and equal to 0.1 the error threshold, which has the same semantics as that of \gridex{}.
%
For our experiments we trained 2 \creepy{} instances, one considering all the 4 input features and the other only the 2 most relevant.

The classification accuracy of each extractor, as well as that of the 9-NN, has been reported in \Cref{tab:experiments}.
%
The Table also shows the number of extracted rules, representing the readability loss $r$ of the extractors.
%
Analogously, the predictive loss $p$ is reported as $1-accuracy$.
%
Finally, the last three columns report the \psifire{} scores associated with each extractor for different values of $\psi$.
%
Data has been gathered on single executions since the goal of this Section is to highlight how to exploit the \fire{} score to carry out comparisons.

A graphical representation of the decision boundaries given by the 9-NN and the extractors are reported in \Cref{fig:experiments}.
%
The bottom row of the Figure reports the isolines for the \psifire{} scores adopted in the experiments.
%
It is important to focus on the fact that it is not possible to obtain less than 3 rules, since the Iris data set describes 3 output classes and in the best case extracted knowledge contains a rule per distinct class.

From \Cref{tab:experiments} and \Cref{fig:experiments} it is evident that \cart{} is the SKE algorithm providing the best output knowledge in terms of both readability and predictive performance.
%
Indeed, it has the lowest \psifire{} score regardless of the adopted $\psi$.
%
This result is true and acceptable since \cart{} is the algorithm providing the smallest amount of rules with the smallest predictive loss.

Different conclusions may be drawn by comparing the \gridex{} instance providing 8 output rules and the 2 \creepy{} instances.
%
Indeed, by observing the corresponding \psifire{} scores and the equivalent isolines in the Figure, \gridex{} may be considered better than one, both or none of the \creepy{} instances when adopting $\psi=1$, 2 or 3, respectively.

{\color{red}
Enlarge the discussion: 
\begin{itemize}
\item We can see the extraction with better readability and predictive performance leads to the lowest FiRe score, but not on the contrary. Besides, the authors do not discuss the situation that both readability and predictive performance are not improved at the same time.
\item If readability and predictive performance are not improved at the same time, is the FiRe score still effective?
\item Only the small Iris dataset was used for the training test, rather than training on the larger dataset. Generally, more knowledge will be extracted with larger datasets, and this part needs further explanation. (RIUSCIAMO A FARE TEST SU ALTRO DATASET?
\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}\label{sec:conclusions}
%%%%%%%%%%%%%%%%%%%%%%%

In this paper we present \fire{}, a scoring function to evaluate and compare SKE algorithms.
%
More precisely, it is a compact score encompassing both a readability assessment and a predictive performance evaluation and it may be exploited to help users choose the best extraction procedure w.r.t.\ a specific fidelity/readability trade-off, expressed as a parameter.
%
The \fire{} score may also be applied together with automatic parameter-tuning procedures.
%
We showed the properties of the scoring function and a rigorous mathematical formulation has been provided.

Our future works will be focused on the enhancement of the \fire{} score concerning its readability loss parameter, with a more expressive formulation than the mere amount of rules provided as output by an extractor.
%
Furthermore, we plan to exploit this score inside the \pedro{} procedure \cite{putciteRefer3person} to automatically find the best parameter values for \gridex{} and \gridrex{} \cite{putcite}.


%%%%%%%%%%%%%%%%%%%%%%%
\section*{Ethics statement}
%%%%%%%%%%%%%%%%%%%%%%%
The authors confirm that proper consideration has been given to any ethics issues. The data sets considered for the experiments do not contain any sensitive features and were treated in compliance with all existing data laws. Being related to the explainability of decisions made by AI systems, the work can have a substantial impact on ethics, ensuring greater transparency and traceability of the decisions made, and guaranteeing equitable and fair approaches. Therefore it could contribute to building more trustable and therefore more ethical systems.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{fire-ijcai-23}

\end{document}

